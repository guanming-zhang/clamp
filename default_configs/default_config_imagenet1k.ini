[INFO]
num_nodes = 1
gpus_per_node = 1
cpus_per_gpu = 1
precision = 16-mixed
fix_random_seed = yes
; change to ddp if multiple_gpus
strategy = ddp

[DATA]
dataset = IMAGENET1K
n_views = 12
;see  https://github.com/vturrisi/solo-learn/blob/main/scripts/pretrain/imagenet/augmentations/asymmetric.yaml
;as a reference
augmentations=RandomResizedCrop,GaussianBlur,RandomGrayscale,ColorJitter,RandomHorizontalFlip,RandomSolarize
crop_size = 224
crop_min_scale = 0.08
crop_max_scale = 1.0
hflip_prob = 0.5
;Gaussian blur
blur_kernel_size = 23
blur_prob = 1.0
grayscale_prob = 0.2
;color jitter
jitter_brightness = 0.4
jitter_contrast = 0.4
jitter_saturation = 0.2
jitter_hue = 0.1
jitter_prob = 0.8
;solarization
solarize_prob = 0.0
imagenet_train_dir = /scratch/work/public/imagenet
imagenet_val_dir = /scratch/gz2241/sig-ml/clap/datasets/imagenet1k_val/val

[SSL]
backbone = resnet50
backbone_out_dim = 2048
use_projection_head = yes
proj_dim = 4096
proj_out_dim = 256
optimizer = LARS
lr = 0.1
lr_scale = linear
momentum = 0.0
weight_decay = 1.5e-6
;for LARS optimizers, if not LARS then lars_eta is redandunt
lars_eta = 0.001 
loss_function = EllipsoidPackingLoss
lw0 = 1.0
lw1 = 1.0
lw2 = 1.0
rs = 3.0
warmup_epochs = 10
n_epochs =  100
batch_size = 2048
save_every_n_epochs = 20
restart_training = no

[LC]
output_dim = 1000
optimizer = SGD
use_batch_norm = no
lr = 0.2
lr_scale = linear
weight_decay = 0.0
momentum = 0.9
loss_function = CrossEntropyLoss
n_epochs = 200
batch_size = 1024
save_every_n_epochs = 50
apply_simple_augmentations = yes
standardize_to_imagenet = yes
restart_training = no

[SemiSL]
loss_function = CrossEntropyLoss
apply_simple_augmentations = yes
standardize_to_imagenet = yes
optimizer = SGD
lr = 0.2
lr_scale = linear
momentum = 0.9
weight_decay = 0.0
n_epochs = 30
batch_size = 1024
save_every_n_epochs = 15
restart_training = no

[TL]
optimizer = SGD
use_batch_norm = no
standardize_to_imagenet = yes
lr = 0.2
lr_scale = linear
weight_decay = 0.0
momentum = 0.9
loss_function = CrossEntropyLoss
n_epochs = 200
batch_size = 1024
save_every_n_epochs = 50
restart_training = no

; some notes
; solo-learn is good reference for hyperparameters
; lr ~ batch_size/256

