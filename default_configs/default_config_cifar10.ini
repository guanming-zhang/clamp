[INFO]
num_nodes = 1
gpus_per_node = 1
cpus_per_gpu = 1
precision = 16-mixed
fix_random_seed = yes
; change to ddp if multiple_gpus
strategy = auto

[DATA]
dataset = CIFAR10
n_views = 12
augmentations=RandomResizedCrop,GaussianBlur,RandomGrayscale,ColorJitter,RandomHorizontalFlip
crop_size = 32
crop_min_scale = 0.08
crop_max_scale = 1.0
hflip_prob = 0.5
blur_kernel_size = 1
blur_prob = 0.5
grayscale_prob = 0.2
jitter_brightness = 0.8
jitter_contrast = 0.8
jitter_saturation = 0.8
jitter_hue = 0.2
jitter_prob = 0.8

[SSL]
backbone = resnet18
backbone_out_dim = 2048
use_projection_head = yes
proj_dim = 2048
proj_out_dim = 256
optimizer = LARS
lr = 0.1
lr_scale = linear
momentum = 0.0
weight_decay = 1e-4
;for LARS optimizers, if not LARS then lars_eta is redandunt
lars_eta = 0.001 
loss_function = EllipsoidPackingLoss
lw0 = 1.0
lw1 = 1.0
lw2 = 1.0
rs = 3.0
warmup_epochs = 10
n_epochs =  100
batch_size = 128
save_every_n_epochs = 20
restart_training = no

[LC]
optimizer = SGD
use_batch_norm = no
lr = 0.1
lr_scale = linear
weight_decay = 1e-4
momentum = 0.0
loss_function = CrossEntropyLoss
n_epochs = 200
save_every_n_epochs = 50
batch_size = 256
apply_simple_augmentations = yes
standardize_to_imagenet = no
restart_training = no
; some notes
; solo-learn is good reference for hyperparameters
; lr ~ batch_size/256

