[INFO]
num_nodes = 1
gpus_per_node = 1
cpus_per_gpu = 4
prefetch_factor = 2
precision = 16-mixed
fix_random_seed = yes
; change to ddp if multiple_gpus
strategy = ddp
if_profile = no

[DATA]
dataset = CIFAR10
n_views = 8
augmentations = RandomResizedCrop,GaussianBlur,RandomGrayscale,ColorJitter,RandomHorizontalFlip
augmentation_package = torchvision
;augmentation_package = albumentations
crop_size = 32
crop_min_scale = 0.08
crop_max_scale = 1.0
hflip_prob = 0.5
blur_kernel_size = 3
blur_prob = 0.5
grayscale_prob = 0.2
jitter_brightness = 0.8
jitter_contrast = 0.8
jitter_saturation = 0.8
jitter_hue = 0.2
jitter_prob = 0.8

[SSL]
backbone = resnet18
backbone_out_dim = 2048
use_projection_head = yes
proj_dim = 2048
proj_out_dim = 256
optimizer = LARS
lr = 0.1
lr_scale = linear
lr_scheduler = cosine-warmup
grad_accumulation_steps = 1
momentum = 0.0
weight_decay = 1e-4
;for LARS optimizers, if not LARS then lars_eta is redandunt
lars_eta = 0.001 
loss_function = EllipsoidPackingLoss
lw0 = 1.0
lw1 = 1.0
lw2 = 1.0
pot_pow = 2.0
rs = 3.0
warmup_epochs = 10
n_epochs =  1000
batch_size = 128
save_every_n_epochs = 100
restart_training = no

[LC]
output_dim = 10
optimizer = SGD
use_batch_norm = no
lr = 0.2
lr_scale = linear
lr_scheduler = cosine
weight_decay = 0.0
momentum = 0.9
loss_function = CrossEntropyLoss
n_epochs = 200
save_every_n_epochs = 50
batch_size = 256
apply_simple_augmentations = yes
standardize_to_imagenet = no
restart_training = no
; some notes
; solo-learn is good reference for hyperparameters
; lr ~ batch_size/256

