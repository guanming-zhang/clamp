[INFO]
num_nodes = 1
gpus_per_node = 1
cpus_per_gpu = 12
prefetch_factor = 2
precision = 16-mixed
fix_random_seed = yes
strategy = ddp
if_profile = no

[DATA]
dataset = IMAGENET1K
n_views = 4
n_trans = 2
augmentation_package = albumentations
augmentations = RandomResizedCrop,GaussianBlur,RandomGrayscale,ColorJitter,RandomHorizontalFlip,RandomSolarize
crop_size = 224,224
crop_min_scale = 0.08,0.08
crop_max_scale = 1.0,1.0
hflip_prob = 0.5,0.5
blur_kernel_size = 23,23
blur_prob = 1.0,0.1
grayscale_prob = 0.2,0.2
jitter_brightness = 0.4,0.4
jitter_contrast = 0.4,0.4
jitter_saturation = 0.2,0.2
jitter_hue = 0.1,0.1
jitter_prob = 0.8,0.8
solarize_prob = 0.0,0.2
imagenet_train_dir = /home/richard/Documents/code/clap/datasets/imagenet1k/train/train.lmdb
imagenet_val_dir = /home/richard/Documents/code/clap/datasets/imagenet1k/val/val.lmdb

[SSL]
backbone = resnet50
use_projection_head = yes
proj_dim = 8192,8192
proj_out_dim = 512
optimizer = LARS
lr = 8.0
lr_scale = linear
lr_scheduler = cosine-warmup
grad_accumulation_steps = 1
momentum = 0.0
weight_decay = 1e-06
exclude_bn_bias_from_weight_decay = yes
lars_eta = 0.001
loss_function = LogRepulsiveEllipsoidPackingLossUnitNorm
lw0 = 0.0
lw1 = 1.0
lw2 = 0.0
max_mem_size = 0
rs = 7.0
pot_pow = 2.0
max_grad_norm = -1.0
warmup_epochs = 10
n_epochs = 100
restart_epochs = -1
batch_size = 8
save_every_n_epochs = 10
restart_training = no

[LC]
output_dim = 1000
optimizer = SGD
use_batch_norm = no
lr_sweep = 0.1,0.2,0.4,0.8,1.6
lr_scale = linear
lr_scheduler = cosine
weight_decay = 1e-6
momentum = 0.9
loss_function = CrossEntropyLoss
n_epochs = 100
batch_size = 2048
save_every_n_epochs = 25
apply_simple_augmentations = yes
standardize_to_imagenet = yes
restart_training = no

[SemiSL]
loss_function = CrossEntropyLoss
apply_simple_augmentations = yes
standardize_to_imagenet = yes
optimizer = SGD
lr_sweep = 0.005,0.01,0.05,0.1,0.5
lr_scale = linear
lr_scheduler = cosine
backbone_lr_slowdown = 0.1
momentum = 0.9
weight_decay = 0.0
n_epochs = 2
batch_size = 8
save_every_n_epochs = 10
restart_training = no

[TL]
optimizer = SGD
use_batch_norm = no
standardize_to_imagenet = yes
lr_sweep = 1e-3,5e-3,1e-2,5e-2,1e-1,5e-1
lr_scale = linear
lr_scheduler = cosine
weight_decay = 0.0
momentum = 0.9
loss_function = CrossEntropyLoss
n_epochs = 200
batch_size = 8
save_every_n_epochs = 50
restart_training = no

