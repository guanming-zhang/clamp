{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fdc4670-6643-4540-9fd9-3e9c5edb4c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from utils import data_utils\n",
    "import helper\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import training_utils\n",
    "from utils import data_utils\n",
    "import torch\n",
    "from model import models\n",
    "import json\n",
    "import os\n",
    "from model import lightning_models\n",
    "import math\n",
    "import pytorch_lightning as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b548de5c-edbb-4da6-8e44-3f8e45615cd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default settings...\n",
      "[SemiSL]does not exist in the config file\n",
      "[TL]does not exist in the config file\n",
      "[KNN]does not exist in the config file\n",
      "[SemiSL]does not exist in the config file\n",
      "[TL]does not exist in the config file\n",
      "[KNN]does not exist in the config file\n",
      "[INFO]\n",
      "num_nodes = 1\n",
      "gpus_per_node = 1\n",
      "cpus_per_gpu = 4\n",
      "prefetch_factor = 2\n",
      "precision = 16-mixed\n",
      "fix_random_seed = True\n",
      "strategy = ddp\n",
      "if_profile = False\n",
      "\n",
      "[DATA]\n",
      "dataset = CIFAR10\n",
      "n_views = 4\n",
      "n_trans = 2\n",
      "augmentations = ['RandomResizedCrop', 'GaussianBlur', 'RandomGrayscale', 'ColorJitter', 'RandomHorizontalFlip', 'RandomSolarize']\n",
      "augmentation_package = albumentations\n",
      "crop_size = [32, 32]\n",
      "crop_min_scale = [0.08, 0.08]\n",
      "crop_max_scale = [1.0, 1.0]\n",
      "hflip_prob = [0.5, 0.5]\n",
      "blur_kernel_size = [3, 3]\n",
      "blur_prob = [0.9, 0.1]\n",
      "grayscale_prob = [0.2, 1.0]\n",
      "jitter_brightness = [0.8, 0.8]\n",
      "jitter_contrast = [0.8, 0.8]\n",
      "jitter_saturation = [0.8, 0.8]\n",
      "jitter_hue = [0.2, 0.2]\n",
      "jitter_prob = [0.8, 0.8]\n",
      "solarize_prob = [1.0, 0.2]\n",
      "\n",
      "[SSL]\n",
      "backbone = resnet18\n",
      "use_projection_head = True\n",
      "proj_dim = [4096]\n",
      "proj_out_dim = 512\n",
      "optimizer = LARS\n",
      "lr = 12.2\n",
      "lr_scale = linear\n",
      "lr_scheduler = cosine-restart\n",
      "grad_accumulation_steps = 1\n",
      "momentum = 0.0\n",
      "weight_decay = 0.0001\n",
      "lars_eta = 0.001\n",
      "loss_function = RepulsiveEllipsoidPackingLossUnitNorm\n",
      "lw0 = 0.0\n",
      "lw1 = 1.0\n",
      "lw2 = 0.0\n",
      "pot_pow = 1.0\n",
      "restart_epochs = 4\n",
      "rs = 3.0\n",
      "warmup_epochs = -1\n",
      "n_epochs = 3\n",
      "batch_size = 8\n",
      "save_every_n_epochs = 1\n",
      "restart_training = False\n",
      "max_grad_norm = -1.0\n",
      "max_mem_size = 64\n",
      "\n",
      "[LC]\n",
      "output_dim = 10\n",
      "optimizer = Adam\n",
      "use_batch_norm = False\n",
      "lr_sweep = [0.3, 0.1, 0.05]\n",
      "lr_scale = linear\n",
      "lr_scheduler = cosine\n",
      "weight_decay = 0.0\n",
      "momentum = -1.0\n",
      "loss_function = CrossEntropyLoss\n",
      "n_epochs = 4\n",
      "save_every_n_epochs = 2\n",
      "batch_size = 1024\n",
      "apply_simple_augmentations = True\n",
      "standardize_to_imagenet = False\n",
      "restart_training = False\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# save the starting time as the last line\\ncurrent_datetime,est_zone = helper.get_est_time_now()\\nif os.path.isfile(os.path.join(\"./simulation_imagenet\",\"starting-time.txt\")):\\n    with open(os.path.join(\"./simulation_imagenet\",\"starting-time.txt\"),\"a\") as f:\\n        f.write(\"\\n\")\\n        f.write(current_datetime.strftime(\"%Y-%m-%d %H:%M:%S\"))\\nelse:\\n    with open(os.path.join(\"./simulation_imagenet\",\"starting-time.txt\"),\"a\") as f:\\n        f.write(current_datetime.strftime(\"%Y-%m-%d %H:%M:%S\"))\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = helper.Config(\"./simulations\",default_config_file=\"./default_configs/default_config_cifar10.ini\")\n",
    "\n",
    "#config = helper.Config(\"./simulation_imagenet\",default_config_file=\"./default_configs/default_config_imagenet1k.ini\")\n",
    "\n",
    "if config.INFO[\"fix_random_seed\"]:\n",
    "    pl.seed_everything(137) # To be reproducable\n",
    "'''\n",
    "# save the starting time as the last line\n",
    "current_datetime,est_zone = helper.get_est_time_now()\n",
    "if os.path.isfile(os.path.join(\"./simulation_imagenet\",\"starting-time.txt\")):\n",
    "    with open(os.path.join(\"./simulation_imagenet\",\"starting-time.txt\"),\"a\") as f:\n",
    "        f.write(\"\\n\")\n",
    "        f.write(current_datetime.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "else:\n",
    "    with open(os.path.join(\"./simulation_imagenet\",\"starting-time.txt\"),\"a\") as f:\n",
    "        f.write(current_datetime.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72a2a26b-d596-42f5-85d0-40e7bfe4ac4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5625\n",
      "625\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for multi-gpu trainning, effective batch size = batch_size*num_gpus\n",
    "ssl_batch_size = config.SSL[\"batch_size\"] // (config.INFO[\"num_nodes\"]*config.INFO[\"gpus_per_node\"]*config.SSL[\"grad_accumulation_steps\"])\n",
    "ssl_train_loader,ssl_test_loader,ssl_val_loader = data_utils.get_dataloader(config.DATA,ssl_batch_size,\n",
    "                                                                                num_workers = config.INFO[\"cpus_per_gpu\"],\n",
    "                                                                                standardized_to_imagenet=True,\n",
    "                                                                                augment_val_set = True,\n",
    "                                                                                prefetch_factor=config.INFO[\"prefetch_factor\"],\n",
    "                                                                                aug_pkg = config.DATA[\"augmentation_package\"])\n",
    "# test_loader and val_loader are not necessary\n",
    "del ssl_test_loader\n",
    "imgs,labels = next(iter(ssl_train_loader))\n",
    "img_list, label_list = [],[]\n",
    "for i_view in range(2):\n",
    "    for j_img in range(2):\n",
    "        img_list.append(imgs[i_view][j_img])\n",
    "        #label_list.append(classes[labels[i_view][j_img]])\n",
    "data_utils.show_images(img_list,2,2,label_list)\n",
    "print(len(ssl_train_loader))\n",
    "print(len(ssl_val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "337f8759-849a-4733-a6d0-d0319d708929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_mem_size is dummy for RepulsiveEllipsoidPackingLossUnitNorm\n",
      "lw2 is dummy for RepulsiveEllipsoidPackingLossUnitNorm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# check if the sub module is the same as needed\\nfor name, module in ssl_model.backbone.named_modules():\\n    print(name, \":\", module)\\n# check if the bachnorm is correctly converted to sync batchnorm\\n\\n\\nssl_model.backbone = torch.nn.SyncBatchNorm.convert_sync_batchnorm(ssl_model.backbone)\\nfor name, module in ssl_model.backbone.named_modules():\\n    if isinstance(module, torch.nn.BatchNorm2d):\\n        print(f\"No BatchNorm2d NOT converted at: {name}\")\\n    elif isinstance(module, torch.nn.SyncBatchNorm):\\n        print(f\"Yes SyncBatchNorm converted at: {name}\")\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if config.SSL[\"lr_scale\"] == \"linear\":\n",
    "    ssl_lr = config.SSL[\"lr\"]*config.SSL[\"batch_size\"]/256.0 # lr ~ 0.1\n",
    "elif config.SSL[\"lr_scale\"] == \"sqrt\":\n",
    "    ssl_lr = config.SSL[\"lr\"]*math.sqrt(config.SSL[\"batch_size\"]) # lr ~ 0.05\n",
    "if \"CIFAR\" in config.DATA[\"dataset\"] or \"MNIST\" in config.DATA[\"dataset\"]:\n",
    "    prune_backbone = True\n",
    "else:\n",
    "    prune_backbone = False\n",
    "ssl_model = lightning_models.CLAP(backbone_name = config.SSL[\"backbone\"],\n",
    "                                  prune = prune_backbone,\n",
    "                                  use_projection_head=config.SSL[\"use_projection_head\"],\n",
    "                                  proj_dim = config.SSL[\"proj_dim\"],\n",
    "                                  proj_out_dim = config.SSL[\"proj_out_dim\"],\n",
    "                                  loss_name= config.SSL[\"loss_function\"],\n",
    "                                  optim_name = config.SSL[\"optimizer\"],\n",
    "                                  lr = ssl_lr,\n",
    "                                  scheduler_name = config.SSL[\"lr_scheduler\"],\n",
    "                                  momentum = config.SSL[\"momentum\"],\n",
    "                                  weight_decay = config.SSL[\"weight_decay\"],\n",
    "                                  eta = config.SSL[\"lars_eta\"],\n",
    "                                  warmup_epochs = config.SSL[\"warmup_epochs\"],\n",
    "                                  n_epochs = config.SSL[\"n_epochs\"],\n",
    "                                  restart_epochs = config.SSL[\"restart_epochs\"],\n",
    "                                  n_views = config.DATA[\"n_views\"],\n",
    "                                  batch_size = ssl_batch_size,\n",
    "                                  lw0 = config.SSL[\"lw0\"],\n",
    "                                  lw1 = config.SSL[\"lw1\"],\n",
    "                                  lw2 = config.SSL[\"lw2\"],\n",
    "                                  max_mem_size = config.SSL[\"max_mem_size\"],\n",
    "                                  rs = config.SSL[\"rs\"],\n",
    "                                  pot_pow = config.SSL[\"pot_pow\"])\n",
    "'''\n",
    "# check if the sub module is the same as needed\n",
    "for name, module in ssl_model.backbone.named_modules():\n",
    "    print(name, \":\", module)\n",
    "# check if the bachnorm is correctly converted to sync batchnorm\n",
    "\n",
    "\n",
    "ssl_model.backbone = torch.nn.SyncBatchNorm.convert_sync_batchnorm(ssl_model.backbone)\n",
    "for name, module in ssl_model.backbone.named_modules():\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "        print(f\"No BatchNorm2d NOT converted at: {name}\")\n",
    "    elif isinstance(module, torch.nn.SyncBatchNorm):\n",
    "        print(f\"Yes SyncBatchNorm converted at: {name}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3edafb7-64e3-4c18-876f-a3900954832e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/richard/miniconda3/envs/dl_env/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/richard/Documents/code/clap/simulations/ssl exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type        | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | backbone | BackboneNet | 15.4 M | train\n",
      "-------------------------------------------------\n",
      "15.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "15.4 M    Total params\n",
      "61.471    Total estimated model params size (MB)\n",
      "73        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82867e5dc8e4e4ca24d6b5b7c26a229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_mem_size is dummy for RepulsiveEllipsoidPackingLossUnitNorm\n",
      "lw2 is dummy for RepulsiveEllipsoidPackingLossUnitNorm\n"
     ]
    }
   ],
   "source": [
    "ssl_dir = os.path.join(config.loc,\"ssl\")\n",
    "if not os.path.isdir(ssl_dir):\n",
    "    os.makedirs(ssl_dir)\n",
    "ssl_model = lightning_models.train_clap(model=ssl_model, \n",
    "                                        train_loader = ssl_train_loader,\n",
    "                                        val_loader = ssl_val_loader,\n",
    "                                        max_grad_norm=config.SSL[\"max_grad_norm\"],\n",
    "                                        max_epochs=config.SSL[\"n_epochs\"],\n",
    "                                        every_n_epochs = config.SSL[\"save_every_n_epochs\"],\n",
    "                                        precision = config.INFO[\"precision\"],\n",
    "                                        checkpoint_path=ssl_dir,\n",
    "                                        if_profile=config.INFO[\"if_profile\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58125fe9-0b8d-4f23-87c6-8ce3b733ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_ckpt = os.path.join(ssl_dir,\"last_epoch_backbone_\" + config.SSL[\"backbone\"] +\".ckpt\")\n",
    "if not os.path.isfile(backbone_ckpt):\n",
    "    torch.save(ssl_model.backbone.net.state_dict(),backbone_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1afc30c-f70e-4a4a-b703-93873d8644ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/richard/miniconda3/envs/dl_env/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/richard/Documents/code/clap/simulations/lc/lr_0.3 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type        | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | backbone   | BackboneNet | 11.2 M | train\n",
      "1 | linear_net | Linear      | 5.1 K  | train\n",
      "---------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.696    Total estimated model params size (MB)\n",
      "71        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_mem_size is dummy for RepulsiveEllipsoidPackingLossUnitNorm\n",
      "lw2 is dummy for RepulsiveEllipsoidPackingLossUnitNorm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/richard/miniconda3/envs/dl_env/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (43) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02db29484e7e4cb38aa6458cd3595add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e338d22694fb4acc8bdfba583d8b2f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     batch_test_acc1        0.3689236044883728\n",
      "     batch_test_acc5        0.8627387285232544\n",
      "     batch_test_loss         5.765250205993652\n",
      "        test_acc1           0.3689236044883728\n",
      "        test_acc5           0.8627387285232544\n",
      "        test_loss            5.765250205993652\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/richard/miniconda3/envs/dl_env/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/richard/Documents/code/clap/simulations/lc/lr_0.1 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type        | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | backbone   | BackboneNet | 11.2 M | train\n",
      "1 | linear_net | Linear      | 5.1 K  | train\n",
      "---------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.696    Total estimated model params size (MB)\n",
      "71        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_mem_size is dummy for RepulsiveEllipsoidPackingLossUnitNorm\n",
      "lw2 is dummy for RepulsiveEllipsoidPackingLossUnitNorm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acfa690b8f0445ebbbadb6e02c675f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d627c617e76478ca51f86b295fe649e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Missing folder: ./simulations/lc/lr_0.1/logs/csv/version_0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:141\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(data_fetcher)\n\u001b[0;32m--> 141\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:295\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.on_advance_end\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    293\u001b[0m     call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_model_zero_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 295\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py:178\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py:142\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_dataloader_outputs()\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_run_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py:265\u001b[0m, in \u001b[0;36m_EvaluationLoop.on_run_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# log metrics\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_logger_connector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_eval_end_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_logged_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:150\u001b[0m, in \u001b[0;36m_LoggerConnector.log_eval_end_metrics\u001b[0;34m(self, metrics)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# log all the metrics as a single dict\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:118\u001b[0m, in \u001b[0;36m_LoggerConnector.log_metrics\u001b[0;34m(self, metrics, step)\u001b[0m\n\u001b[1;32m    117\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog_metrics(metrics\u001b[38;5;241m=\u001b[39mscalar_metrics, step\u001b[38;5;241m=\u001b[39mstep)\n\u001b[0;32m--> 118\u001b[0m \u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.12/site-packages/lightning_utilities/core/rank_zero.py:42\u001b[0m, in \u001b[0;36mrank_zero_only.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m default\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.12/site-packages/lightning_fabric/loggers/csv_logs.py:160\u001b[0m, in \u001b[0;36mCSVLogger.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.12/site-packages/pytorch_lightning/loggers/csv_logs.py:65\u001b[0m, in \u001b[0;36mExperimentWriter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m hparams_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNAME_HPARAMS_FILE)\n\u001b[0;32m---> 65\u001b[0m \u001b[43msave_hparams_to_yaml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:329\u001b[0m, in \u001b[0;36msave_hparams_to_yaml\u001b[0;34m(config_yaml, hparams, use_omegaconf)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_dir(fs, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(config_yaml)):\n\u001b[0;32m--> 329\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing folder: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(config_yaml)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# convert Namespace or AD to dict\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Missing folder: ./simulations/lc/lr_0.1/logs/csv/version_0.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 46\u001b[0m\n\u001b[1;32m     32\u001b[0m ssl_model\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39mremove_projection_head()\n\u001b[1;32m     34\u001b[0m lc_model \u001b[38;5;241m=\u001b[39m lightning_models\u001b[38;5;241m.\u001b[39mLinearClassification(\n\u001b[1;32m     35\u001b[0m              backbone \u001b[38;5;241m=\u001b[39m ssl_model\u001b[38;5;241m.\u001b[39mbackbone,\n\u001b[1;32m     36\u001b[0m              in_dim \u001b[38;5;241m=\u001b[39m ssl_model\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39mfeature_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m              weight_decay \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mLC[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     44\u001b[0m              n_epochs \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mLC[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 46\u001b[0m lc_model \u001b[38;5;241m=\u001b[39m \u001b[43mlightning_models\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_lc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinear_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlc_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlc_train_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlc_test_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlc_val_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLC\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevery_n_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLC\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msave_every_n_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlc_sub_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINFO\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrestart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLC\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrestart_training\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# get the best performed one\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(lc_sub_dir,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/Documents/code/clap/model/lightning_models.py:568\u001b[0m, in \u001b[0;36mtrain_lc\u001b[0;34m(linear_model, train_loader, test_loader, val_loader, max_epochs, every_n_epochs, checkpoint_path, num_nodes, gpus_per_node, strategy, precision, restart, if_profile)\u001b[0m\n\u001b[1;32m    566\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mfit(linear_model, train_loader,val_loader,ckpt_path\u001b[38;5;241m=\u001b[39mckpt_files[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 568\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinear_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m test_output \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtest(linear_model,test_loader)\n\u001b[1;32m    570\u001b[0m result \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m:test_output[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    571\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_acc1\u001b[39m\u001b[38;5;124m\"\u001b[39m:test_output[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_acc1\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    572\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_acc5\u001b[39m\u001b[38;5;124m\"\u001b[39m:test_output[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_acc5\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    573\u001b[0m         }\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:67\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m     exit(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m---> 67\u001b[0m     \u001b[43m_interrupt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     trainer\u001b[38;5;241m.\u001b[39m_teardown()\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# teardown might access the stage so we reset it after\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:81\u001b[0m, in \u001b[0;36m_interrupt\u001b[0;34m(trainer, exception)\u001b[0m\n\u001b[1;32m     79\u001b[0m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mon_exception(exception)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m logger \u001b[38;5;129;01min\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mloggers:\n\u001b[0;32m---> 81\u001b[0m     \u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfailed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.12/site-packages/lightning_utilities/core/rank_zero.py:42\u001b[0m, in \u001b[0;36mrank_zero_only.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `rank_zero_only.rank` needs to be set before use\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m default\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.12/site-packages/lightning_fabric/loggers/csv_logs.py:169\u001b[0m, in \u001b[0;36mCSVLogger.finalize\u001b[0;34m(self, status)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# When using multiprocessing, finalize() should be a no-op on the main process, as no experiment has been\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# initialized there\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.12/site-packages/lightning_utilities/core/rank_zero.py:42\u001b[0m, in \u001b[0;36mrank_zero_only.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `rank_zero_only.rank` needs to be set before use\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m default\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.12/site-packages/lightning_fabric/loggers/csv_logs.py:160\u001b[0m, in \u001b[0;36mCSVLogger.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;129m@rank_zero_only\u001b[39m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.12/site-packages/pytorch_lightning/loggers/csv_logs.py:65\u001b[0m, in \u001b[0;36mExperimentWriter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Save recorded hparams and metrics into files.\"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m hparams_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNAME_HPARAMS_FILE)\n\u001b[0;32m---> 65\u001b[0m \u001b[43msave_hparams_to_yaml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl_env/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:329\u001b[0m, in \u001b[0;36msave_hparams_to_yaml\u001b[0;34m(config_yaml, hparams, use_omegaconf)\u001b[0m\n\u001b[1;32m    327\u001b[0m fs \u001b[38;5;241m=\u001b[39m get_filesystem(config_yaml)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_dir(fs, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(config_yaml)):\n\u001b[0;32m--> 329\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing folder: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(config_yaml)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# convert Namespace or AD to dict\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(hparams, Namespace):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Missing folder: ./simulations/lc/lr_0.1/logs/csv/version_0."
     ]
    }
   ],
   "source": [
    "lc_batch_size = config.LC[\"batch_size\"] // (config.INFO[\"num_nodes\"]*config.INFO[\"gpus_per_node\"])\n",
    "data_info = {\"dataset\":config.DATA[\"dataset\"],\"batch_size\":lc_batch_size,\"n_views\":1,\"n_trans\":1,\"augmentations\":[\"RandomResizedCrop\",\"RandomHorizontalFlip\"],\n",
    "            \"crop_size\":[config.DATA[\"crop_size\"][0]],\"crop_min_scale\":[0.08],\"crop_max_scale\":[1.0],\"hflip_prob\":[0.5]}\n",
    "# need to specify the location of the data for imagenet\n",
    "if \"IMAGENET1K\" in config.DATA[\"dataset\"]:\n",
    "    data_info[\"imagenet_train_dir\"] = config.DATA[\"imagenet_train_dir\"]\n",
    "    data_info[\"imagenet_val_dir\"] = config.DATA[\"imagenet_val_dir\"]\n",
    "\n",
    "lc_train_loader,lc_test_loader,lc_val_loader = data_utils.get_dataloader(data_info,lc_batch_size,num_workers=config.INFO[\"cpus_per_gpu\"],\n",
    "                                                                         standardized_to_imagenet=config.LC[\"standardize_to_imagenet\"],\n",
    "                                                                         prefetch_factor=config.INFO[\"prefetch_factor\"])\n",
    "# root directory for linear classification\n",
    "lc_dir = os.path.join(config.loc,\"lc\")\n",
    "if not os.path.isdir(lc_dir):\n",
    "    os.makedirs(lc_dir)\n",
    "if \"lr_sweep\" in config.LC:\n",
    "    lr_list = config.LC[\"lr_sweep\"]\n",
    "else:\n",
    "    lr_list = [config.LC[\"lr\"]]\n",
    "# sweep learning rates\n",
    "best = {\"best_test_acc1\":0.0,\"best_test_acc5\":0.0,\"best_test_loss\":0.0,\"best_model_dir\":\"none\"}\n",
    "for lr in lr_list:\n",
    "    lc_sub_dir = os.path.join(lc_dir,\"lr_{}\".format(lr))\n",
    "    os.makedirs(lc_sub_dir,exist_ok=True)\n",
    "    if config.LC[\"lr_scale\"] == \"linear\":\n",
    "        lc_lr = lr*config.LC[\"batch_size\"]/256.0 # lr ~ 0.1\n",
    "    elif config.LC[\"lr_scale\"] == \"sqrt\":\n",
    "        lc_lr = lr*math.sqrt(config.LC[\"batch_size\"]) # lr ~ 0.05\n",
    "    # load the backbone from the check point\n",
    "    best_ssl_ckpt = os.path.join(ssl_dir,\"best_val.ckpt\")\n",
    "    ssl_model = lightning_models.CLAP.load_from_checkpoint(best_ssl_ckpt)\n",
    "    ssl_model.backbone.remove_projection_head()\n",
    "\n",
    "    lc_model = lightning_models.LinearClassification(\n",
    "                 backbone = ssl_model.backbone,\n",
    "                 in_dim = ssl_model.backbone.feature_dim,\n",
    "                 out_dim = config.LC[\"output_dim\"],\n",
    "                 use_batch_norm = config.LC[\"use_batch_norm\"],\n",
    "                 optim_name = config.LC[\"optimizer\"],\n",
    "                 scheduler_name = config.LC[\"lr_scheduler\"],\n",
    "                 lr = lc_lr, \n",
    "                 momentum = config.LC[\"momentum\"],\n",
    "                 weight_decay = config.LC[\"weight_decay\"],\n",
    "                 n_epochs = config.LC[\"n_epochs\"])\n",
    "    \n",
    "    lc_model = lightning_models.train_lc(linear_model = lc_model,\n",
    "            train_loader = lc_train_loader,\n",
    "            test_loader = lc_test_loader,\n",
    "            val_loader = lc_val_loader,\n",
    "            max_epochs = config.LC[\"n_epochs\"],\n",
    "            every_n_epochs = config.LC[\"save_every_n_epochs\"],\n",
    "            checkpoint_path = lc_sub_dir,\n",
    "            precision = config.INFO[\"precision\"],\n",
    "            restart = config.LC[\"restart_training\"])\n",
    "    # get the best performed one\n",
    "    with open(os.path.join(lc_sub_dir,\"results.json\")) as f:\n",
    "        result = json.load(f)\n",
    "    if result[\"test_acc1\"] > best[\"best_test_acc1\"]:\n",
    "        best[\"best_test_acc1\"] = result[\"test_acc1\"] \n",
    "        best[\"best_test_acc5\"] = result[\"test_acc5\"] \n",
    "        best[\"best_test_loss\"] = result[\"test_loss\"]\n",
    "        best[\"best_model_dir\"] = lc_sub_dir\n",
    "#save the information about the best model\n",
    "with open(os.path.join(lc_dir,\"results.json\"),\"w\") as f:\n",
    "    json.dump(best,f,indent=4)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6823b6cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lc_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlc_dir\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lc_dir' is not defined"
     ]
    }
   ],
   "source": [
    "lc_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88681e77-5ab3-4401-8d14-34606a63fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune or semi-supervised learning\n",
    "if len(config.SemiSL) > 0:\n",
    "    semisl_batch_size = config.SemiSL[\"batch_size\"] // (config.INFO[\"num_nodes\"]*config.INFO[\"gpus_per_node\"])\n",
    "    for dataset in [\"IMAGENET1K-1percent\",\"IMAGENET1K-10percent\"]:\n",
    "        data_info = {\"dataset\":dataset,\"batch_size\":semisl_batch_size,\"n_views\":1,\"augmentations\":[\"RandomResizedCrop\",\"RandomHorizontalFlip\"],\n",
    "                 \"crop_size\":config.DATA[\"crop_size\"],\"crop_min_scale\":0.08,\"crop_max_scale\":1.0,\"hflip_prob\":0.5}\n",
    "        # add the location for imagenet dataset\n",
    "        data_info[\"imagenet_train_dir\"] = config.DATA[\"imagenet_train_dir\"]\n",
    "        data_info[\"imagenet_val_dir\"] = config.DATA[\"imagenet_val_dir\"]\n",
    "        \n",
    "        semisl_train_loader,semisl_test_loader,semisl_val_loader = data_utils.get_dataloader(data_info,semisl_batch_size,num_workers=config.INFO[\"cpus_per_gpu\"],\n",
    "                                                                                 standardized_to_imagenet=config.SemiSL[\"standardize_to_imagenet\"],\n",
    "                                                                                 prefetch_factor=config.INFO[\"prefetch_factor\"])\n",
    "        semisl_dir = os.path.join(config.loc,\"semisl-\"+dataset)\n",
    "        if not os.path.isdir(semisl_dir):\n",
    "            os.makedirs(semisl_dir)\n",
    "        if \"lr_sweep\" in config.SemiSL:\n",
    "            lr_list = config.SemiSL[\"lr_sweep\"]\n",
    "        else:\n",
    "            lr_list = [config.SemiSL[\"lr\"]]\n",
    "        # sweep learning rates\n",
    "        best = {\"best_test_acc1\":0.0,\"best_test_acc5\":0.0,\"best_test_loss\":0.0,\"best_model_dir\":\"none\"}\n",
    "        for lr in lr_list:\n",
    "            semisl_sub_dir = os.path.join(semisl_dir,\"lr_{}\".format(lr))\n",
    "            os.makedirs(semisl_sub_dir,exist_ok=True)\n",
    "            if config.SemiSL[\"lr_scale\"] == \"linear\":\n",
    "                semisl_lr = lr*config.SemiSL[\"batch_size\"]/256.0 # lr ~ 0.1\n",
    "            elif config.SemiSL[\"lr_scale\"] == \"sqrt\":\n",
    "                semisl_lr = lr*math.sqrt(config.SemiSL[\"batch_size\"]) # lr ~ 0.05\n",
    "            # load the backbone from the checkpoint\n",
    "            best_ssl_ckpt = os.path.join(ssl_dir,\"best_val.ckpt\")\n",
    "            ssl_model = lightning_models.CLAP.load_from_checkpoint(best_ssl_ckpt)\n",
    "            ssl_model.backbone.remove_projection_head()\n",
    "            # load the best linear classifier from the checkpoint\n",
    "            with open(os.path.join(lc_dir,\"results.json\")) as f:\n",
    "                results = json.load(f)\n",
    "                best_lc_dir = results[\"best_model_dir\"] \n",
    "            lc_model = lightning_models.LinearClassification.load_from_checkpoint(os.path.join(best_lc_dir,\"best_val.ckpt\"),backbone = ssl_model.backbone)\n",
    "            semisl_model = lightning_models.FineTune(backbone = ssl_model.backbone,\n",
    "                    linear_net= lc_model.linear_net,\n",
    "                    optim_name = config.SemiSL[\"optimizer\"],\n",
    "                    lr = semisl_lr, \n",
    "                    momentum = config.SemiSL[\"momentum\"],\n",
    "                    weight_decay = config.SemiSL[\"weight_decay\"],\n",
    "                    n_epochs = config.SemiSL[\"n_epochs\"])\n",
    "            semisl_model = lightning_models.train_finetune(\n",
    "                    finetune_model = semisl_model,\n",
    "                    train_loader = semisl_test_loader,\n",
    "                    test_loader = semisl_test_loader,\n",
    "                    val_loader = semisl_val_loader,\n",
    "                    max_epochs = config.SemiSL[\"n_epochs\"],\n",
    "                    every_n_epochs = config.SemiSL[\"save_every_n_epochs\"],\n",
    "                    checkpoint_path = semisl_sub_dir,\n",
    "                    precision= config.INFO[\"precision\"],\n",
    "                    restart = config.SemiSL[\"restart_training\"])\n",
    "            # get the best performed one\n",
    "            with open(os.path.join(semisl_sub_dir,\"results.json\")) as f:\n",
    "                result = json.load(f)\n",
    "            if result[\"test_acc1\"] > best[\"best_test_acc1\"]:\n",
    "                best[\"best_test_acc1\"] = result[\"test_acc1\"] \n",
    "                best[\"best_test_acc5\"] = result[\"test_acc5\"] \n",
    "                best[\"best_test_loss\"] = result[\"test_loss\"]\n",
    "                best[\"best_model_dir\"] = semisl_sub_dir\n",
    "        #save the information about the best model\n",
    "        with open(os.path.join(semisl_dir,\"results.json\"),\"w\") as f:\n",
    "            json.dump(best,f,indent=4)  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2438876b-2b1f-46c4-b757-3ec4a21db478",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Transfer learning(freeze backbone)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39mTL) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      3\u001b[0m     tl_output_dim \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCIFAR100\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      4\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFOOD101\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m101\u001b[39m,\n\u001b[1;32m      5\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFLOWERS102\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m102\u001b[39m}\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCIFAR100\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFOOD101\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFLOWERS102\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "# Transfer learning(freeze backbone)\n",
    "if len(config.TL) > 0:\n",
    "    tl_output_dim = {\"CIFAR100\":100,\n",
    "                    \"FOOD101\":101,\n",
    "                    \"FLOWERS102\":102}\n",
    "    for dataset in [\"CIFAR100\",\"FOOD101\",\"FLOWERS102\"]:\n",
    "        tl_batch_size = config.TL[\"batch_size\"] // (config.INFO[\"num_nodes\"]*config.INFO[\"gpus_per_node\"])\n",
    "        data_info = {\"dataset\":dataset,\"batch_size\":semisl_batch_size,\"n_views\":1,\"augmentations\":[\"RandomResizedCrop\",\"RandomHorizontalFlip\"],\n",
    "                 \"crop_size\":config.DATA[\"crop_size\"],\"crop_min_scale\":0.08,\"crop_max_scale\":1.0,\"hflip_prob\":0.5}\n",
    "        tl_train_loader,tl_test_loader,tl_val_loader = data_utils.get_dataloader(data_info,lc_batch_size,num_workers=config.INFO[\"cpus_per_gpu\"],\n",
    "                                                                                 standardized_to_imagenet=config.TL[\"standardize_to_imagenet\"],\n",
    "                                                                                 prefetch_factor=config.INFO[\"prefetch_factor\"])\n",
    "        tl_dir = os.path.join(config.loc,\"tl-\"+dataset)\n",
    "        if not os.path.isdir(tl_dir):\n",
    "            os.makedirs(tl_dir)\n",
    "        if \"lr_sweep\" in config.SemiSL:\n",
    "            lr_list = config.SemiSL[\"lr_sweep\"]\n",
    "        else:\n",
    "            lr_list = [config.SemiSL[\"lr\"]]\n",
    "        # sweep learning rates\n",
    "        best = {\"best_test_acc1\":0.0,\"best_test_acc5\":0.0,\"best_test_loss\":0.0,\"best_model_dir\":\"none\"}\n",
    "        for lr in lr_list:\n",
    "            tl_sub_dir = os.path.join(tl_dir,\"lr_{}\".format(lr))\n",
    "            os.makedirs(tl_sub_dir,exist_ok=True)\n",
    "            if config.TL[\"lr_scale\"] == \"linear\":\n",
    "                tl_lr = lr*config.TL[\"batch_size\"]/256.0 # lr ~ 0.1\n",
    "            elif config.TL[\"lr_scale\"] == \"sqrt\":\n",
    "                tl_lr = lr*math.sqrt(config.TL[\"batch_size\"]) # lr ~ 0.05\n",
    "            # load the backbone from the checkpoint\n",
    "            best_ssl_ckpt = os.path.join(ssl_dir,\"best_val.ckpt\")\n",
    "            ssl_model = lightning_models.CLAP.load_from_checkpoint(best_ssl_ckpt)\n",
    "            ssl_model.backbone.remove_projection_head()\n",
    "        \n",
    "            tl_model = lightning_models.LinearClassification(\n",
    "                    backbone = ssl_model.backbone,\n",
    "                    in_dim = ssl_model.backbone.feature_dim,\n",
    "                    out_dim = tl_output_dim[dataset],\n",
    "                    use_batch_norm = config.TL[\"use_batch_norm\"],\n",
    "                    optim_name = config.TL[\"optimizer\"],\n",
    "                    lr = tl_lr, \n",
    "                    scheduler_name= config.TL[\"lr_scheduler\"],\n",
    "                    momentum = config.TL[\"momentum\"],\n",
    "                    weight_decay = config.TL[\"weight_decay\"],\n",
    "                    n_epochs = config.TL[\"n_epochs\"])\n",
    "\n",
    "            tl_model = lightning_models.train_lc(\n",
    "                    linear_model = tl_model,\n",
    "                    train_loader = tl_train_loader,\n",
    "                    val_loader = tl_val_loader,\n",
    "                    test_loader = tl_test_loader,\n",
    "                    every_n_epochs = config.TL[\"save_every_n_epochs\"],\n",
    "                    max_epochs = config.TL[\"n_epochs\"],\n",
    "                    precision = config.INFO[\"precision\"],\n",
    "                    checkpoint_path = tl_sub_dir,\n",
    "                    restart = config.LC[\"restart_training\"]) \n",
    "                        # get the best performed one\n",
    "            with open(os.path.join(tl_sub_dir,\"results.json\")) as f:\n",
    "                result = json.load(f)\n",
    "            if result[\"test_acc1\"] > best[\"best_test_acc1\"]:\n",
    "                best[\"best_test_acc1\"] = result[\"test_acc1\"] \n",
    "                best[\"best_test_acc5\"] = result[\"test_acc5\"] \n",
    "                best[\"best_test_loss\"] = result[\"test_loss\"]\n",
    "        #save the information about the best model\n",
    "        with open(os.path.join(tl_dir,\"results.json\"),\"w\") as f:\n",
    "            json.dump(best,f,indent=4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fae7a09-5dca-41ef-8191-59393d06a19d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
